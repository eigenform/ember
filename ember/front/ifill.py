from amaranth import *
from amaranth.lib.enum import *
from amaranth.lib.data import *
from amaranth.lib.wiring import *
import amaranth.lib.memory as memory
from amaranth.utils import exact_log2, ceil_log2

from amaranth_soc.wishbone import Interface as WishboneInterface
from amaranth_soc.wishbone import Signature as WishboneSignature

from ember.common import *
from ember.common.pipeline import *
from ember.common.xbar import SimpleCrossbar
from ember.common.coding import EmberPriorityEncoder, ChainedPriorityEncoder
from ember.riscv.paging import *
from ember.param import *
from ember.front.l1i import L1ICacheWritePort
from ember.uarch.front import *
from ember.sim.fakeram import *

class L1IFillSource(Enum, shape=2):
    NONE     = 0
    DEMAND   = 1
    PREFETCH = 2

class L1IFillPort(Signature):
    """ L1I fill unit port. 
    """

    class Request(Signature):
        """ L1 instruction cache fill request.

        .. note::
            Right now, the way index specified by a request is unused,
            and the way used when writing the L1I is generated very late in 
            the process (within in the L1ICache module itself). 

            Currently, the replacement policy implementation for the L1I
            is very ad-hoc. You might want to keep this wire around in case 
            it's useful to select a particular way index earlier in the 
            pipeline. This will probably become clearer when we decide to 
            implement a policy that isn't random. 

        Members
        =======
        valid: 
            This request is valid
        addr: 
            Physical address for this request
        way: 
            Target way in the L1I cache
        ftq_idx:
            Index of the FTQ entry that generated the request
            
        """
        def __init__(self, p: EmberParams):
            super().__init__({
                "valid": Out(1),
                "addr": Out(p.paddr),
                "blocks": Out(p.fblk_size_shape),
                "way": Out(ceil_log2(p.l1i.num_ways)),
                "ftq_idx": Out(p.ftq.index_shape),
                "src": Out(L1IFillSource)
            })

    class Response(Signature):
        """ L1I instruction cache fill response. 

        Members
        =======
        valid:
            This response is valid
        ftq_idx:
            Index of the FTQ entry that generated the request
        way:
            Way index used for the request
        """
        def __init__(self, p: EmberParams):
            super().__init__({
                "valid": Out(1),
                "ftq_idx": Out(p.ftq.index_shape),
                "way": Out(ceil_log2(p.l1i.num_ways)),
                "src": Out(L1IFillSource),
            })

    def __init__(self, p: EmberParams):
        super().__init__({
            "req": Out(self.Request(p)),
            "resp": In(self.Response(p)),
        })





class L1IFillStatus(Signature):
    def __init__(self, p: EmberParams):
        super().__init__({
            "ready":  Out(1),
        })

class L1IMshrState(Enum, shape=2):
    """ The state associated with an MSHR. 

    Values
    ======
    IDLE:
        No request is being serviced.
    RUN:
        The request is being serviced.
    """
    IDLE  = 0
    RUN   = 1


class L1IMissStatusHoldingRegister(Component):
    """ L1I cache "miss-status holding register" (MSHR)

    The L1I fill unit includes MSHRs that are used to track fill requests 
    generated by both demand fetch and prefetch requests that have missed in 
    the L1I cache. An MSHR holds the request while data is being received 
    from memory and written back to the L1I cache data/tag arrays. 

    An MSHR involves *pipelined* interactions with the L1I cache and 
    memory interface. Ideally, this allows data from an external memory
    to be streamed into the L1I cache. 

    An MSHR moves through the following sequence of states:

    - ``L1IMshrState.IDLE``: Ready to accept a request
    - ``L1IMshrState.RUN``: Request is registered and being sent to memory

    .. warning::
        Currently, this module does *not* handle requests to memory that 
        cannot be completed in a single cycle. 

    Ports
    =====
    ready: 
        High when this MSHR is ready to accept a request
    complete:
        High when this MSHR can be reset
    req: 
        Incoming fill request to this MSHR
    l1i_wp: 
        L1I cache write port interface
    fakeram: 
        Memory interface

    """
    def __init__(self, param: EmberParams):
        self.p = param
        self.stage = PipelineStages()

        self.r_state     = Signal(L1IMshrState, init=L1IMshrState.IDLE)
        self.r_busy      = Signal(init=0)
        self.r_base_addr = Signal(self.p.paddr)
        self.r_ftq_idx   = Signal(self.p.ftq.index_shape)
        self.r_way       = Signal(ceil_log2(self.p.l1i.num_ways))
        self.r_blocks    = Signal(self.p.fblk_size_shape)
        self.r_src       = Signal(L1IFillSource)
        self.r_blk  = Signal(4)
        self.r_addr = Signal(self.p.paddr)


        # Memory access
        self.stage.add_stage(1, {
            "addr": self.p.paddr,
            "blk": unsigned(4),
        })

        # Memory response
        self.stage.add_stage(2, {
            "addr": self.p.paddr,
            "blk": unsigned(4),
        })

        # L1I writeback
        self.stage.add_stage(3, {
            "addr": self.p.paddr,
            "blk": unsigned(4),
            "data": ArrayLayout(unsigned(32), param.l1i.line_depth),
        })


        signature = Signature({
            "ready": Out(1),
            "complete": In(1),
            "port": In(L1IFillPort(param)),
            "l1i_wp": Out(L1ICacheWritePort(param)),
            "fakeram": Out(FakeRamInterface(param.l1i.line_depth)),
        })
        super().__init__(signature)
        return

    def elaborate_s0(self, m: Module):
        m.d.comb += self.ready.eq(self.r_state == L1IMshrState.IDLE)

        m.d.sync += [
            self.port.resp.valid.eq(0),
            self.port.resp.ftq_idx.eq(0),
            self.port.resp.way.eq(0),
            self.port.resp.src.eq(0),
        ]

        with m.Switch(self.r_state):
            with m.Case(L1IMshrState.IDLE):
                with m.If(self.port.req.valid):
                    m.d.sync += [
                        Assert(self.port.req.blocks != 0),
                        self.r_state.eq(L1IMshrState.RUN),
                        self.r_busy.eq(1),
                        self.r_base_addr.eq(self.port.req.addr),
                        self.r_ftq_idx.eq(self.port.req.ftq_idx),
                        self.r_way.eq(self.port.req.way),
                        self.r_blocks.eq(self.port.req.blocks),
                        self.r_src.eq(self.port.req.src),

                        self.r_blk.eq(1),
                        self.r_addr.eq(self.port.req.addr),

                        self.stage[1].addr.eq(self.port.req.addr),
                        self.stage[1].blk.eq(1),
                        self.stage[1].valid.eq(1),
                    ]
            with m.Case(L1IMshrState.RUN):
                m.d.sync += [
                    self.stage[1].addr.eq(0),
                    self.stage[1].blk.eq(0),
                    self.stage[1].valid.eq(0),
                ]
                done = (self.r_blk == self.r_blocks)
                with m.If(~done):
                    next_blk = (self.r_blk + 1)
                    next_addr = (self.r_addr.bits + self.p.l1i.line_bytes)
                    m.d.sync += [
                        self.r_blk.eq(next_blk),
                        self.r_addr.eq(next_addr),
                        self.stage[1].addr.eq(next_addr),
                        self.stage[1].blk.eq(next_blk),
                        self.stage[1].valid.eq(1),
                    ]


    def elaborate_s1(self, m: Module):
        m.d.comb += [
            self.fakeram.req.addr.eq(0),
            self.fakeram.req.valid.eq(0),
        ]
        m.d.sync += [
            self.stage[2].addr.eq(0),
            self.stage[2].blk.eq(0),
            self.stage[2].valid.eq(0),
        ]

        with m.If(self.stage[1].valid):
            m.d.comb += [
                self.fakeram.req.addr.eq(self.stage[1].addr),
                self.fakeram.req.valid.eq(1),
            ]
            m.d.sync += [
                self.stage[2].addr.eq(self.stage[1].addr),
                self.stage[2].blk.eq(self.stage[1].blk),
                self.stage[2].valid.eq(1),
            ]

    def elaborate_s2(self, m: Module):
        m.d.sync += [
            self.stage[3].addr.eq(0),
            self.stage[3].blk.eq(0),
            self.stage[3].valid.eq(0),
            self.stage[3].data.eq(0),
        ]

        # FIXME: For now, we're *assuming* that a memory device will always
        # reply on the cycle immediately following the request. 
        with m.If(self.stage[2].valid):
            m.d.sync += Assert(self.fakeram.resp.valid)

        with m.If(self.stage[2].valid & self.fakeram.resp.valid):
            m.d.sync += [
                self.stage[3].addr.eq(self.stage[2].addr),
                self.stage[3].blk.eq(self.stage[2].blk),
                self.stage[3].valid.eq(self.stage[2].valid),
                self.stage[3].data.eq(Cat(*self.fakeram.resp.data)),
            ]

    def elaborate_s3(self, m: Module):
        m.d.comb += [
            self.l1i_wp.req.valid.eq(0),
            self.l1i_wp.req.set.eq(0),
            self.l1i_wp.req.way.eq(0),
            self.l1i_wp.req.line_data.eq(0),
            self.l1i_wp.req.tag_data.eq(0),
            self.l1i_wp.req.tag_data.valid.eq(0),
        ]
        with m.If(self.stage[3].valid):
            m.d.comb += [
                self.l1i_wp.req.valid.eq(1),
                self.l1i_wp.req.set.eq(self.stage[3].addr.l1i.set),
                self.l1i_wp.req.way.eq(self.r_way),
                self.l1i_wp.req.line_data.eq(self.stage[3].data),
                self.l1i_wp.req.tag_data.ppn.eq(self.stage[3].addr.sv32.ppn),
                self.l1i_wp.req.tag_data.valid.eq(1),
            ]

        done = (self.stage[3].blk == self.r_blocks)
        with m.If(done & self.stage[3].valid):
            m.d.sync += [
                self.r_state.eq(L1IMshrState.IDLE),
                self.r_busy.eq(0),
                self.r_base_addr.eq(0),
                self.r_ftq_idx.eq(0),
                self.r_way.eq(0),
                self.r_blocks.eq(0),
                self.r_src.eq(0),
                self.r_blk.eq(0),
                self.r_addr.eq(0),
                self.port.resp.valid.eq(1),
                self.port.resp.ftq_idx.eq(self.r_ftq_idx),
                self.port.resp.way.eq(self.r_way),
                self.port.resp.src.eq(self.r_src),
            ]

    def elaborate(self, platform):
        m = Module()
        self.elaborate_s0(m)
        self.elaborate_s1(m)
        self.elaborate_s2(m)
        self.elaborate_s3(m)
        return m

class NewL1IFillUnit(Component):
    def __init__(self, param: EmberParams):
        self.p = param
        self.num_mshr = param.l1i.fill.num_mshr
        self.num_port = param.l1i.fill.num_port
        signature = Signature({
            "port":     In(L1IFillPort(param)).array(self.num_port),
            "sts":     Out(L1IFillStatus(param)),
            "l1i_wp":  Out(L1ICacheWritePort(param)).array(self.num_mshr),
            "fakeram": Out(FakeRamInterface(param.l1i.line_depth)).array(self.num_mshr),
        })
        super().__init__(signature)

    def elaborate(self, platform):
        m = Module()

        # Logic for mapping requests to available MSHRs
        xbar = m.submodules.xbar = SimpleCrossbar(self.num_port, self.num_mshr)
        xbar_resp = m.submodules.xbar_resp = \
                SimpleCrossbar(self.num_port, self.num_mshr)

        # Instantiate MSHRs
        mshr = []
        for idx in range(self.num_mshr):
            x = m.submodules[f"mshr{idx}"] = L1IMissStatusHoldingRegister(self.p)
            mshr.append(x)

        # Connect MSHRs to resources
        # FIXME: This simply *assumes* that the L1I cache and memory interface
        # have multiple ports
        for idx in range(self.num_mshr):
            connect(m, mshr[idx].l1i_wp, flipped(self.l1i_wp[idx]))
            connect(m, mshr[idx].fakeram, flipped(self.fakeram[idx]))

        # Distribute valid requests to available MSHRs
        port_req_valid = [ port.req.valid for port in self.port ]
        mshr_ready    = [ m.ready for m in mshr ]
        m.d.comb += [
            xbar.upstream_grant.eq(Cat(*port_req_valid)),
            xbar.downstream_grant.eq(Cat(*mshr_ready)),
            self.sts.ready.eq(Cat(*mshr_ready).any()),
        ]
        # Distribute responses from completed MSHRs to output ports
        mshr_resp_valid = [ m.port.resp.valid for m in mshr ]
        m.d.comb += [
            xbar_resp.upstream_grant.eq(C(3, self.num_port)),
            xbar_resp.downstream_grant.eq(Cat(*mshr_resp_valid)),
        ]


        # Where 'N' is the number of ports, and 'M' is the number of MSHRs: 
        #
        #                 N-to-N            N-to-M           M-to-M         
        #  self.port.req|=======>|req_in  |========|req_out|=======>|mshr.req
        #               |        |        |  xbar  |       |        |
        #               |        |        |        |       |        |          
        #               |        |        |  xbar  |       |        |
        # self.port.resp|<=======|resp_out|========|resp_in|<=======|mshr.resp
        #                 N-to-N            N-to-M           M-to-M
        #


        # Connect ports to wires
        req_in = Array(L1IFillPort.Request(self.p).flip().create() for _ in range(self.num_port))
        resp_out = Array(L1IFillPort.Response(self.p).create() for _ in range(self.num_port))
        for port_idx in range(self.num_port):
            connect(m, flipped(self.port[port_idx].req), req_in[port_idx])
            connect(m, resp_out[port_idx], flipped(self.port[port_idx].resp))

        # Connect wires to MSHRs
        req_out = Array(L1IFillPort.Request(self.p).create() for _ in range(self.num_mshr))
        resp_in = Array(L1IFillPort.Response(self.p).create() for _ in range(self.num_mshr))
        for mshr_idx in range(self.num_mshr):
            connect(m, req_out[mshr_idx], mshr[mshr_idx].port.req)
            connect(m, mshr[mshr_idx].port.resp, flipped(resp_in[mshr_idx]))

        # Default assignments
        for idx in range(self.num_port):
            m.d.comb += [
                req_out[idx].valid.eq(0),
                req_out[idx].addr.eq(0),
                req_out[idx].way.eq(0),
                req_out[idx].ftq_idx.eq(0),
                req_out[idx].blocks.eq(0),
                resp_out[idx].valid.eq(0),
                resp_out[idx].ftq_idx.eq(0),
                resp_out[idx].src.eq(L1IFillSource.NONE),
            ]

        for idx in range(self.num_port):
            with m.If(xbar.grant[idx]): 
                mshr_idx = xbar.dst_idx[idx]
                m.d.comb += [
                    #Print(Format("port_req{} -> mshr_req{}", idx, mshr_idx)),
                    req_out[mshr_idx].valid.eq(req_in[idx].valid),
                    req_out[mshr_idx].addr.eq(req_in[idx].addr),
                    req_out[mshr_idx].way.eq(req_in[idx].way),
                    req_out[mshr_idx].ftq_idx.eq(req_in[idx].ftq_idx),
                    req_out[mshr_idx].src.eq(req_in[idx].src),
                    req_out[mshr_idx].blocks.eq(req_in[idx].blocks),
                ]

        for idx in range(self.num_port):
            with m.If(xbar_resp.grant[idx]): 
                mshr_idx = xbar_resp.dst_idx[idx]
                m.d.comb += [
                    #Print(Format("mshr_resp{} -> port_resp{}", mshr_idx, idx)),
                    resp_out[idx].valid.eq(resp_in[mshr_idx].valid),
                    resp_out[idx].ftq_idx.eq(resp_in[mshr_idx].ftq_idx),
                    resp_out[idx].src.eq(resp_in[mshr_idx].src),
                ]

        return m

